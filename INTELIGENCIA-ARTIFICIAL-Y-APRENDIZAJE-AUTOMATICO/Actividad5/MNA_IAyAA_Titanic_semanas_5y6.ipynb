{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MNA - IAyAA**\n",
        "## **Sesión-Active Class Complementaria**\n",
        "## **Semanas 5 y 6**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://www.kaggle.com/datasets/yasserh/titanic-dataset"
      ],
      "metadata": {
        "id": "zozPejRcWme2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4arRj0QkaK75"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "# Si queremos asegurar que en la partición los niveles de las categóricas queden estratificados lo mejor posible:\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "Q2MYJBBraPgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanicdata = sns.load_dataset('titanic')\n",
        "df = pd.DataFrame(titanicdata)   # por facilidad consideremos los datps como un DataFrame de Pandas.\n",
        "print(df.shape)\n",
        "df.head().T"
      ],
      "metadata": {
        "id": "-P-SKApxaRlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "w6PySDRhEqj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables con la misma información: \"survived\" y \"alive\"; \"pclass\" y \"class\"; \"embarked\" y \"embark_town\".\n",
        "\n"
      ],
      "metadata": {
        "id": "h1vuBHj1Lcnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['alive','class','embark_town',\n",
        "         'deck'\n",
        "         ], axis=1, inplace=True)\n",
        "print(df.shape)\n",
        "df.head().T"
      ],
      "metadata": {
        "id": "XsqwVsVDKNXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PREGUNTA:**\n",
        "\n",
        "## **¿Cómo se afecta el entrenamiento el usar por ejemplo una partición de 60%-20%-20% o una de 80%-10%-10%? ¿Cuándo se recomendaría usar una u otra de estas particiones?**\n"
      ],
      "metadata": {
        "id": "cBbHX5vx2VAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtv, ytrain, ytv = train_test_split(df.iloc[:,1:], df.iloc[:,0],\n",
        "                                            train_size=0.6,\n",
        "                                            random_state=0\n",
        "                                            )\n",
        "Xval, Xtest, yval, ytest = train_test_split(Xtv, ytv, train_size=0.5, shuffle=True,\n",
        "                                            random_state=0\n",
        "                                            )\n",
        "\n",
        "print(Xtrain.shape, ytrain.shape)\n",
        "print(Xval.shape, yval.shape)\n",
        "print(Xtest.shape, ytest.shape)\n",
        "\n",
        "\n",
        "# Sin cancelar \"deck\":\n",
        "#print('\\nSolo para ilustrar que en este proceso es donde se pueden generar niveles con 0 valores en alguna variable categórica:')\n",
        "#print(Xtrain['deck'].value_counts())\n",
        "#print(Xval['deck'].value_counts())\n",
        "#print(Xtest['deck'].value_counts())"
      ],
      "metadata": {
        "id": "N9-yGJWaciRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separemos variables numéricas y categóricas.\n",
        "# Supongamos que las separamos como se indica a continuación.\n",
        "\n",
        "cat_ord_lista = ['pclass']    # lista de las variables ordinales\n",
        "\n",
        "cat_nom_lista = ['sex','adult_male','embarked','who','alone']   # lista de variables nominales(& binarias)\n",
        "\n",
        "num_lista = ['age','sibsp','parch','fare']    # lista de las variables numéricas."
      ],
      "metadata": {
        "id": "BhEQDk7icnap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Análisis Variables Categóricas**"
      ],
      "metadata": {
        "id": "_GH_cuzJ4YZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xtrain['pclass'].value_counts())\n",
        "print(Xtrain['pclass'].value_counts() / Xtrain.shape[0])"
      ],
      "metadata": {
        "id": "vfe-ICCqNxjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xtrain['sex'].value_counts())\n",
        "print(Xtrain['sex'].value_counts() / Xtrain.shape[0])"
      ],
      "metadata": {
        "id": "dktOpKe54XNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xtrain['adult_male'].value_counts())\n",
        "print(Xtrain['adult_male'].value_counts() / Xtrain.shape[0])"
      ],
      "metadata": {
        "id": "EvOnaw6y4hph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xtrain['embarked'].value_counts())\n",
        "print(Xtrain['embarked'].value_counts() / Xtrain.shape[0])"
      ],
      "metadata": {
        "id": "l3ZfPSTYDnhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xtrain['who'].value_counts())\n",
        "print(Xtrain['who'].value_counts() / Xtrain.shape[0])"
      ],
      "metadata": {
        "id": "hrqTJpzADncF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xtrain['alone'].value_counts())\n",
        "print(Xtrain['alone'].value_counts() / Xtrain.shape[0])"
      ],
      "metadata": {
        "id": "J7vc-qxZOYtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y la variable de salida:\n",
        "\n",
        "tmp = pd.DataFrame(yval)\n",
        "\n",
        "print(tmp.value_counts())\n",
        "print(tmp.value_counts() / tmp.shape[0])"
      ],
      "metadata": {
        "id": "Jd0nEigAYX1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PREGUNTA:**\n",
        "\n",
        "## **¿Qué transformaciones consideras serían adecuadas aplicar a las categóricas, de ser el caso?**"
      ],
      "metadata": {
        "id": "JRSawKK_6P77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Análisis Variables Numéricas**"
      ],
      "metadata": {
        "id": "9XAbtXJS5Fwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain[num_lista].hist();"
      ],
      "metadata": {
        "id": "8JiwDvuxDnXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain[num_lista].boxplot();"
      ],
      "metadata": {
        "id": "jkkUVuM2O2Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PREGUNTA:**\n",
        "\n",
        "## **¿Qué transformaciones consideras serían adecuadas aplicar a las numéricas, de ser el caso?**"
      ],
      "metadata": {
        "id": "MvN79LbZ6jai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipe = Pipeline(steps = [('impMediana', SimpleImputer(strategy='median')),\n",
        "                             ('minmax', MinMaxScaler())])\n",
        "num_pipe_nombres = num_lista\n",
        "\n",
        "catord_pipe = Pipeline(steps = [('impOrd', SimpleImputer(strategy='most_frequent')),\n",
        "                             ('ordtrasnf', OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1))])\n",
        "catord_pipe_nombres = cat_ord_lista\n",
        "\n",
        "catnom_pipe = Pipeline(steps = [('impModa', SimpleImputer(strategy='most_frequent')),\n",
        "                             ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))])\n",
        "catnom_pipe_nombres = cat_nom_lista\n",
        "\n",
        "\n",
        "columnasTransformer = ColumnTransformer(transformers = [('num_transf', num_pipe, num_pipe_nombres),\n",
        "                                                        ('catord_transf', catord_pipe, catord_pipe_nombres),\n",
        "                                                        ('catnom_transf', catnom_pipe, catnom_pipe_nombres)\n",
        "                                                        ],\n",
        "                                        remainder='passthrough')\n"
      ],
      "metadata": {
        "id": "F-pDuL5BDnTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **¿Qué otras transformaciones consideras se pudieran agregar?**"
      ],
      "metadata": {
        "id": "9bFI0WIsAb2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conjuntamos conjunto de Entrenamiento y Validación para usar Cross-Validation:\n",
        "\n",
        "Xtrainval = pd.concat([Xtrain, Xval], axis=0)\n",
        "ytrainval = pd.concat([ytrain, yval], axis=0)\n",
        "\n",
        "print(Xtrainval.shape, ytrainval.shape)"
      ],
      "metadata": {
        "id": "53a6Rs3T6mFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **¿Por qué concatenamos estos conjuntos?**"
      ],
      "metadata": {
        "id": "pK7noTBDA-oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos verifcar la cantidad de columnas que se estarán gnerando después de aplicar las transformaciones:\n",
        "\n",
        "Xtmp = Xtrainval.copy()\n",
        "tmp = columnasTransformer.fit_transform(Xtmp)\n",
        "\n",
        "print(\"Antes de las transformaciones:\", Xtmp.shape)\n",
        "print(\"Después de las transformaciones:\", tmp.shape)"
      ],
      "metadata": {
        "id": "qGld6zYQ6mCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelos de Regresión:\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.neighbors import KNeighborsRegressor\n",
        "#from sklearn.tree import DecisionTreeRegressor\n",
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "#from xgboost import XGBRegressor\n",
        "#from sklearn.neural_network import MLPRegressor\n",
        "#from sklearn.svm import SVR\n",
        "\n",
        "# Modelos de Claisficación:\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks"
      ],
      "metadata": {
        "id": "mwrZBkknBlzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost-Clasificación:\n",
        "\n",
        "https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
        "\n",
        "XGBost-Regresión:\n",
        "\n",
        "https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
        "\n",
        "\n",
        "Puedes consultar la lista de métricas en la siguiente liga:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ],
      "metadata": {
        "id": "QgP9XcG1aQvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PREGUNTA:**\n",
        "\n",
        "## **Ejecuta primeramente el modelo con sus hiperparámetros predeterminados y luego trata de ir razonando cómo y cuáles modificar.**\n",
        "\n",
        "## **En general se hace uso de métodos como GridSearchCV() o bien RandomizedSearchCV() de Scikit-learn para la búsqueda de los mejores hiperparámetros, pero por el momento hagámoslo de manera \"manual\" para tener una mejor comprensión del ejecto de cada uno.**\n",
        "\n",
        "## **Pogamos por el momento que el objetivo de este ejercicio sea que cada modelo no quede ni sub-entrenado, ni sobre-entrenado con respecto a la métrica de la Exactitud (Accuracy), sin importar las demás métricas. Es decir, a partir de los hiperparámetros predetermiandos de cada modelo, trata de lograr dicho objetivo.**\n",
        "\n",
        "## **Y antes de iniciar, ¿podríamos establecer un primer valor de entrada de la Exactitud (Accuracy) a partir del cual pudiéramos decir que el modelo está sub-entrenado? Indica qué valor tomarás como referencia en este problema para determinar que un modelo está sub-entrenado: ... [incluye tu respuesta] ...**\n",
        "\n",
        "## **Y para el caso sobre-entrenado, ¿cuándo podríamos decir que el modelo está sobre-emntrenado? Indica qué criterio tomarás como referencia en este problema para determinar que un modelo está sobre-entrenado: ... [incluye tu respuesta] ...**"
      ],
      "metadata": {
        "id": "y_gECwMdg2gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos algunas variantes de los hiperparámetros de cada modelo\n",
        "# para que te familiarices con la manera en que afecta cada uno\n",
        "# de ellos en su desempeño:\n",
        "\n",
        "def modelos():\n",
        "  modelos, nombres = list(), list()\n",
        "\n",
        "\n",
        "\n",
        "  # Regresión Logística:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "  modelos.append(LogisticRegression(max_iter=100,    # 10, 100, 1000\n",
        "                                    C = 1.0,     # 0.000001, 0.005, 0.01,  1000\n",
        "                                    #random_state=1\n",
        "                                    ))\n",
        "  nombres.append('LR')\n",
        "\n",
        "\n",
        "\n",
        "  # k-Vecinos Más Cercanos:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "  modelos.append(KNeighborsClassifier(n_neighbors = 5,     # 1, 5, 21, 201\n",
        "                                      ))\n",
        "  nombres.append('kNN')\n",
        "\n",
        "\n",
        "\n",
        "  # Árbol de Decisión:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "  modelos.append(DecisionTreeClassifier(max_depth = None,       # None, 5, 3, 1\n",
        "                                        min_samples_split=2,   # 2,3,5, 20\n",
        "                                        #min_samples_leaf=1,  # trata al inicio de usar solo uno de estos, split o leaf, para su mejor comprensión.\n",
        "                                        #random_state=7\n",
        "                                        ))\n",
        "  nombres.append('DTree')\n",
        "\n",
        "\n",
        "\n",
        "  # Bosque Aleatorio:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "  modelos.append(RandomForestClassifier(n_estimators= 100,    # 100\n",
        "                                        max_depth= None,      # None, 1, 2,3, 4, 5, 6 ... ¿Se esperaría la misma profunidad en un RF y en un DT?\n",
        "                                        min_samples_split=2,    # 2, 5, 15\n",
        "                                        #random_state=0\n",
        "                                        ))\n",
        "  nombres.append('RF')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # XGBoost:\n",
        "  # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
        "  # https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "  modelos.append(XGBClassifier(booster= 'gbtree',\n",
        "                               n_estimators=100,   # A medida que se aumente \"n_estimators\", se debe disminuir el \"learning_rate\", de manera general.\n",
        "                               max_depth= 6,             # 1,3, 6,\n",
        "                               learning_rate=0.3,   #  0.3, 0.000001, 0.01, 100,     # participación o peso de cada árbol desde el inicio.\n",
        "                               subsample=1.0,        # 1.0,  0.9, 0.8, ... 0.5    # submuestreo con respecto a los renglones para evitar overfitting.\n",
        "                               #random_state=5,\n",
        "                               objective='binary:logistic',\n",
        "                               n_jobs=-1))\n",
        "  nombres.append('XGBoost')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Red Neuronal Artificial: Perceptrón MultiCapa:\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "  modelos.append(MLPClassifier(hidden_layer_sizes=(30,),     # 100,  6, 24, 30, (15,15), (50,50)\n",
        "                               activation='logistic',\n",
        "                               max_iter=1000,                  # 200,\n",
        "                               alpha=0.0001,               # término de regularización L2.\n",
        "                               #learning_rate='constant',       # tasa de aprendizaje o tamaño de paso del método Gradiente Descendente.\n",
        "                               #learning_rate_init=0.001,\n",
        "                               #random_state=1\n",
        "                               ))\n",
        "  nombres.append('MLP')\n",
        "\n",
        "\n",
        "\n",
        "  # Máquina de Vector Soporte : Support Vector Machine\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "  modelos.append(SVC(kernel='rbf',\n",
        "                     C= 1.0,      # 1.0, 0.00001, 1000           # inversamente proporcional a la constante de regularización L2.\n",
        "                     gamma= 'scale',           # scale,  0.005\n",
        "                     #class_weight='balanced',     # Siempre puedes hacer uso del balanceo en caso de que ayude.\n",
        "                     #random_state=7\n",
        "                     ))\n",
        "  nombres.append('SVM')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return modelos, nombres\n",
        "\n",
        "\n",
        "\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "modelo, nombres = modelos()\n",
        "resultados = list()\n",
        "\n",
        "for i in range(len(modelo)):\n",
        "\n",
        "  # Definimos nuestro pipeline con las transformaciones y los modelos:\n",
        "  pipeline = Pipeline(steps=[('ct',columnasTransformer),('m',modelo[i])])\n",
        "\n",
        "  # Aplicaremos validación-cruzada:\n",
        "  micv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3)\n",
        "\n",
        "  # Definimos las métricas que desamos recuperar:\n",
        "  mismetricas = {'accuracy','precision','recall','f1','roc_auc'}   # métricas clasificación\n",
        "  #mismetricas = {'neg_mean_squared_error','neg_root_mean_squared_error','neg_mean_absolute_percentage_error','r2'}  # métricas regresión\n",
        "\n",
        "  # Llevamos a cabo el entrenamiento:\n",
        "  scores = cross_validate(pipeline,\n",
        "                          Xtrainval,\n",
        "                          ytrainval,\n",
        "                          scoring=mismetricas,\n",
        "                          cv=micv,\n",
        "                          return_train_score=True,\n",
        "                          )\n",
        "\n",
        "  # Guardemos el resultado de cada modelopara análisis posteriores.\n",
        "  resultados.append(scores)\n",
        "\n",
        "  # Desplegamos los valores de las métricas para verificar si no hay\n",
        "  # subentrenamiento o sobreentrenamiento:\n",
        "  print('>> %s' % nombres[i])\n",
        "  for j,k in enumerate(list(scores.keys())):\n",
        "    if j>1:\n",
        "      print('\\t %s %.3f (%.3f)' % (k, np.mean(scores[k]),np.std(scores[k])))\n",
        "\n"
      ],
      "metadata": {
        "id": "E0PjacenAFOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = [resultados[j]['test_precision'] for j in range(len(resultados)) ]\n",
        "\n",
        "plt.boxplot(tmp, labels=nombres, showmeans=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24FIrWS9AFFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mejor Modelo:**\n",
        "\n",
        "Supongamos que fue la SVM con la métrica de Precision:"
      ],
      "metadata": {
        "id": "KhC5Gnxi5OPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mf = SVC(kernel='rbf',C= 1.0,gamma= 'scale')\n",
        "\n",
        "mf_pipe = Pipeline(steps=[('ct',columnasTransformer),('m',mf)])\n",
        "\n",
        "mf_pipe.fit(Xtrainval,ytrainval)\n",
        "\n",
        "# PROFESOR:\n",
        "# Verificar que utilicen el conjunto de Prueba Test:\n",
        "yhat = mf_pipe.predict(Xtest)\n",
        "\n",
        "print(classification_report(ytest, yhat))\n",
        "\n",
        "print('Métrica-ROC-Test:', np.round(roc_auc_score(ytest, yhat), 2))"
      ],
      "metadata": {
        "id": "ibRneJO92VBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O bien, usando la matriz de confusión y calculando directamente cada métrica:\n",
        "\n",
        "cm = confusion_matrix(ytest, yhat)\n",
        "cm"
      ],
      "metadata": {
        "id": "Bf5qiXIM6g-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VN = cm[0,0]\n",
        "VP = cm[1,1]\n",
        "FN = cm[1,0]\n",
        "FP = cm[0,1]\n",
        "\n",
        "mi_precision = VP / (VP + FP)\n",
        "mi_precision"
      ],
      "metadata": {
        "id": "Xx7T1T156tyi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}